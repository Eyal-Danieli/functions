{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is to create a function to ingest data from snowflake with a Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dask frameworks enables users to parallelize their python code and run it as a distributed process on Iguazio cluster and dramatically accelerate their performance. <br>\n",
    "In this notebook we'll create an mlrun function running as a dask client to ingest data from snowflake. <br>\n",
    "It also demonstrates how to run parallelize query against snowflake using Dask Delayed option to query a large data set from snowflake. <br>\n",
    "The function will be published on the function marketplace. <br>\n",
    "For more information on dask over kubernetes: https://kubernetes.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-03-17 17:11:56,500 [info] loaded project snowflake-dask from MLRun DB\n",
      "artifact_path = ('snowflake-dask', '/v3io/projects/snowflake-dask')\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "project_name = \"snowflake-dask\"\n",
    "dask_cluster_name=\"snowflake-dask-cluster\"\n",
    "artifact_path = mlrun.set_environment(project=project_name,\n",
    "                                      artifact_path = os.path.join(os.path.abspath('/v3io/projects/'), project_name))\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f'artifact_path = {artifact_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load snowflake configuration from config file. \n",
    "This is for demo purpose, in the real production code, you would need to put the snowflake connection info into secrets use the secrets in the running pod to connect to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf77378.eu-west-2.aws\n"
     ]
    }
   ],
   "source": [
    "# Load connection info\n",
    "with open(\".config.yaml\") as f:\n",
    "    connection_info = yaml.safe_load(f)\n",
    "\n",
    "# verify the config\n",
    "print(connection_info['account'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function querys data from snowflake using snowflake python connector for parallel processing of the query results. <br>\n",
    "With snoeflake python connector, when you execute a query, the cursor will return the result batches. <br>\n",
    "Using Dask Delayed it will return and process results set in parallel. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the function to a py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting snowflake_dask.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile snowflake_dask.py\n",
    "\"\"\"Snowflake Dask - Ingest Snaowflake data with Dask\"\"\"\n",
    "import warnings\n",
    "import mlrun\n",
    "from mlrun.execution import MLClientCtx\n",
    "import snowflake.connector as snow\n",
    "from dask.distributed import Client\n",
    "from dask.dataframe import from_delayed\n",
    "from dask import delayed\n",
    "from dask import dataframe as dd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@delayed\n",
    "def load(batch):\n",
    "\n",
    "    \"\"\"A delayed load one batch.\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"BATCHING\")\n",
    "        df_ = batch.to_pandas()\n",
    "        return df_\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {batch} for {e}\")\n",
    "        raise\n",
    "\n",
    "def load_results(context: MLClientCtx,\n",
    "                 dask_client: str,\n",
    "                 connection_info: str,\n",
    "                 query: str,\n",
    "                 parquet_out_dir = None,\n",
    "                 publish_name = None\n",
    "                ) -> None:\n",
    "\n",
    "    \"\"\"Snowflake Dask - Ingest Snaowflake data with Dask\n",
    "\n",
    "    :param context:           the function context\n",
    "    :param dask_client:       dask cluster function name\n",
    "    :param connection_info:   Snowflake database connection info (this wikk be in a secret later)\n",
    "    :param query:             query to for Snowflake\n",
    "    :param parquet_out_dir:   directory path for the output parquet files\n",
    "                              (default None, not write out)\n",
    "    :param publish_name:      name of the dask dataframe to publish to the dask cluster\n",
    "                              (default None, not publish)\n",
    "\n",
    "    \"\"\"\n",
    "    context = mlrun.get_or_create_ctx('snawflake-dask-cluster')\n",
    "\n",
    "    # setup dask client from the MLRun dask cluster function\n",
    "    if dask_client:\n",
    "        client = mlrun.import_function(dask_client).client\n",
    "        context.logger.info(f'Existing dask client === >>> {client}\\n')\n",
    "    else:\n",
    "        client = Client()\n",
    "        context.logger.info(f'\\nNewly created dask client === >>> {client}\\n')\n",
    "\n",
    "    conn = snow.connect(**connection_info)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query)\n",
    "    batches = cur.get_result_batches()\n",
    "    context.logger.info(f'batches len === {len(batches)}\\n')\n",
    "\n",
    "    dfs = []\n",
    "    for batch in batches:\n",
    "        if batch.rowcount > 0:\n",
    "            df = load(batch)\n",
    "            dfs.append(df)\n",
    "    ddf = from_delayed(dfs)\n",
    "\n",
    "    # materialize the query results set for some sample compute\n",
    "\n",
    "    ddf_describe = ddf.describe().compute()\n",
    "\n",
    "    context.logger.info(f'query  === >>> {query}\\n')\n",
    "    context.logger.info(f'ddf  === >>> {ddf}\\n')\n",
    "    context.log_result('number of rows', len(ddf.index))\n",
    "    context.log_dataset(\"ddf_describe\", df=ddf_describe)\n",
    "\n",
    "    if publish_name:\n",
    "        context.log_result('data_set_name', publish_name)\n",
    "        if not client.list_datasets():\n",
    "            ddf.persist(name = publish_name)\n",
    "            client.publish_dataset(publish_name=ddf)\n",
    "\n",
    "    if parquet_out_dir:\n",
    "        dd.to_parquet(df=ddf, path=parquet_out_dir)\n",
    "        context.log_result('parquet directory', parquet_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the code to MLRun function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code_to_function to convert the code to MLRun <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-03-17 17:11:56,703 [info] Started building image: .mlrun/func-snowflake-dask-snowflake-dask:latest\n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest mlrun/mlrun:0.10.0 \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image mlrun/mlrun:0.10.0 from registry index.docker.io \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest mlrun/mlrun:0.10.0 \n",
      "\u001b[36mINFO\u001b[0m[0000] Returning cached image manifest              \n",
      "\u001b[36mINFO\u001b[0m[0000] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Unpacking rootfs as cmd RUN python -m pip install bokeh snowflake-connector-python[pandas] requires it. \n",
      "\u001b[36mINFO\u001b[0m[0019] RUN python -m pip install bokeh snowflake-connector-python[pandas] \n",
      "\u001b[36mINFO\u001b[0m[0019] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0036] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0036] args: [-c python -m pip install bokeh snowflake-connector-python[pandas]] \n",
      "\u001b[36mINFO\u001b[0m[0036] Running: [/bin/sh -c python -m pip install bokeh snowflake-connector-python[pandas]] \n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/site-packages (2.4.2)\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-2.7.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/site-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/site-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/site-packages (from bokeh) (9.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/site-packages (from bokeh) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/site-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/site-packages (from bokeh) (1.21.5)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.3)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.15.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.0.11)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.27.1)\n",
      "Collecting pycryptodomex!=3.5.0,<4.0.0,>=3.2\n",
      "  Downloading pycryptodomex-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: cryptography<37.0.0,>=3.1.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.3.2)\n",
      "Collecting oscrypto<2.0.0\n",
      "  Downloading oscrypto-1.2.1-py2.py3-none-any.whl (192 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2021.10.8)\n",
      "Requirement already satisfied: setuptools>34.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (57.4.0)\n",
      "Collecting pyOpenSSL<22.0.0,>=16.2.0\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Collecting pyarrow<6.1.0,>=6.0.0\n",
      "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "Requirement already satisfied: pandas<1.4.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.3.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.21)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/site-packages (from cryptography<37.0.0,>=3.1.0->snowflake-connector-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=16.8->bokeh) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas<1.4.0,>=1.0.0->snowflake-connector-python[pandas]) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (1.26.8)\n",
      "Installing collected packages: asn1crypto, pyOpenSSL, pycryptodomex, oscrypto, snowflake-connector-python, pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "storey 0.10.5 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "mlrun 0.10.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "Successfully installed asn1crypto-1.5.1 oscrypto-1.2.1 pyOpenSSL-21.0.0 pyarrow-6.0.1 pycryptodomex-3.14.1 snowflake-connector-python-2.7.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0041] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0046] Pushing image to docker-registry.default-tenant.app.us-sales-322.iguazio-cd1.com:80/mlrun/func-snowflake-dask-snowflake-dask:latest \n",
      "\u001b[36mINFO\u001b[0m[0046] Pushed image to 1 destinations               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = mlrun.code_to_function(name=\"snowflake-dask\",  \n",
    "                            kind='job', \n",
    "                            filename='snowflake_dask.py',\n",
    "                            image='mlrun/mlrun',\n",
    "                            requirements='requirements.txt',\n",
    "                            handler=\"load_results\", \n",
    "                            description=\"Snowflake Dask - Ingest snowflake data in parallel with Dask cluster\",\n",
    "                            categories=[\"data-prep\"],\n",
    "                            labels={\"author\": \"xingsheng\"}\n",
    "                           )\n",
    "fn.apply(mlrun.platforms.auto_mount())\n",
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export function to local `function.yaml` file for testing\n",
    "in the real usage, we will import a function from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-03-17 17:12:47,044 [info] function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fae6fe3e690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.export('function.yaml')\n",
    "# print(fn.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import a function from local `function.yaml' for testing (Need to change it to import from hub before PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = mlrun.import_function(\"./function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = mlrun.import_function(\"hub://snowflake_dask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a dask cluster and specify the configuration for the dask process (e.g. replicas, memory etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'db://snowflake-dask/snowflake-dask-cluster'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function URI is db://<project>/<name>\n",
    "dask_uri = f'db://{project_name}/{dask_cluster_name}'\n",
    "dask_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf = mlrun.new_function(name=dask_cluster_name, \n",
    "                         kind='dask', \n",
    "                         image='mlrun/mlrun',\n",
    "                         requirements=[\"bokeh\", \"snowflake-connector-python[pandas]\"]\n",
    "                        )\n",
    "dsf.apply(mlrun.mount_v3io())\n",
    "dsf.spec.remote = True\n",
    "dsf.spec.min_replicas = 1\n",
    "dsf.spec.max_replicas = 10\n",
    "dsf.spec.service_type = \"NodePort\"\n",
    "dsf.with_requests(mem='4G', cpu='2')\n",
    "# dsf.spec.node_port=30088\n",
    "# dsf.spec.scheduler_timeout = \"5 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-03-17 17:12:47,426 [info] Started building image: .mlrun/func-snowflake-dask-snowflake-dask-cluster:latest\n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest mlrun/mlrun:0.10.0 \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image mlrun/mlrun:0.10.0 from registry index.docker.io \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest mlrun/mlrun:0.10.0 \n",
      "\u001b[36mINFO\u001b[0m[0000] Returning cached image manifest              \n",
      "\u001b[36mINFO\u001b[0m[0000] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Unpacking rootfs as cmd RUN python -m pip install bokeh snowflake-connector-python[pandas] requires it. \n",
      "\u001b[36mINFO\u001b[0m[0019] RUN python -m pip install bokeh snowflake-connector-python[pandas] \n",
      "\u001b[36mINFO\u001b[0m[0019] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0036] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0036] args: [-c python -m pip install bokeh snowflake-connector-python[pandas]] \n",
      "\u001b[36mINFO\u001b[0m[0036] Running: [/bin/sh -c python -m pip install bokeh snowflake-connector-python[pandas]] \n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/site-packages (2.4.2)\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-2.7.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/site-packages (from bokeh) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/site-packages (from bokeh) (9.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/site-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/site-packages (from bokeh) (1.21.5)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/site-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/site-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.3)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.0.11)\n",
      "Collecting pyOpenSSL<22.0.0,>=16.2.0\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2021.10.8)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.3.0)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2021.3)\n",
      "Requirement already satisfied: cryptography<37.0.0,>=3.1.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.3.2)\n",
      "Requirement already satisfied: setuptools>34.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (57.4.0)\n",
      "Collecting pycryptodomex!=3.5.0,<4.0.0,>=3.2\n",
      "  Downloading pycryptodomex-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.27.1)\n",
      "Collecting oscrypto<2.0.0\n",
      "  Downloading oscrypto-1.2.1-py2.py3-none-any.whl (192 kB)\n",
      "Requirement already satisfied: pandas<1.4.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.3.5)\n",
      "Collecting pyarrow<6.1.0,>=6.0.0\n",
      "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.21)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/site-packages (from cryptography<37.0.0,>=3.1.0->snowflake-connector-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=16.8->bokeh) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas<1.4.0,>=1.0.0->snowflake-connector-python[pandas]) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (1.26.8)\n",
      "Installing collected packages: asn1crypto, pyOpenSSL, pycryptodomex, oscrypto, snowflake-connector-python, pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "storey 0.10.5 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "mlrun 0.10.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "Successfully installed asn1crypto-1.5.1 oscrypto-1.2.1 pyOpenSSL-21.0.0 pyarrow-6.0.1 pycryptodomex-3.14.1 snowflake-connector-python-2.7.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0042] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0046] Pushing image to docker-registry.default-tenant.app.us-sales-322.iguazio-cd1.com:80/mlrun/func-snowflake-dask-snowflake-dask-cluster:latest \n",
      "\u001b[36mINFO\u001b[0m[0046] Pushed image to 1 destinations               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsf.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-03-17 17:13:51,354 [info] trying dask client at: tcp://mlrun-snowflake-dask-cluster-15ea793c-d.default-tenant:8786\n",
      "> 2022-03-17 17:13:51,391 [info] using remote dask scheduler (mlrun-snowflake-dask-cluster-15ea793c-d) at: tcp://mlrun-snowflake-dask-cluster-15ea793c-d.default-tenant:8786\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://default-tenant.app.us-sales-322.iguazio-cd1.com:30088/status\" target=\"_blank\" >dashboard link: default-tenant.app.us-sales-322.iguazio-cd1.com:30088</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = dsf.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the function you would see a remote dashboard link as part of the result. click on this link takes you to the dask monitoring dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'my-local-test'\n",
    "parquet_path = f\"/v3io/bigdata/pq_from_sf_dask/{p}\"\n",
    "\n",
    "fn.run(handler = 'load_results',\n",
    "       params={\"dask_client\": dask_uri, \n",
    "               \"connection_info\": connection_info, \n",
    "               \"query\": \"SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\",\n",
    "               \"parquet_out_dir\": parquet_path,\n",
    "               \"publish_name\": \"customer\",\n",
    "              }\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the progress in the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can view the progress and detailed information in the mlrun UI by clicking on the uid above. <br>\n",
    "Also, to track the dask progress in the dask UI click on the \"dashboard link\" above the \"client\" section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
