{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Batch Predict\n",
    "\n",
    "A function for inferring given input through a given model while producing a **Result Set** and performing **Data Drift Analysis**.\n",
    "\n",
    "In this notebook we will go over the function's docs and outputs and see an end-to-end example of running it.\n",
    "\n",
    "1. [Documentation](#chapter1)\n",
    "2. [Results Prediction](#chapter2)\n",
    "3. [Data Drift Analysis](#chapter3)\n",
    "4. [End-to-end Demo](#chapter4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"chapter1\"></a>\n",
    "## 1. Documentation\n",
    "\n",
    "Perform a prediction on a given dataset with the given model. Can perform drift analysis between the sample set statistics stored in the model to the current input data. The drift rule is the value per-feature mean of the TVD and Hellinger scores according to the thresholds configures here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1. Parameters:\n",
    "* **context**: `mlrun.MLClientCtx`\n",
    "    An MLRun context.\n",
    "* **model**: `str`\n",
    "    The model Store path, a logged model URI.\n",
    "* **dataset**: `Union[mlrun.DataItem, list, dict, pd.DataFrame, pd.Series, np.ndarray]`\n",
    "    The dataset to infer through the model.\n",
    "    * Can be passed in `inputs` as either a Dataset artifact / Feature vector URI.\n",
    "    * Or, in `parameters` as a list, dictionary or numpy array.\n",
    "* **drop_columns**: `Union[str, List[str], int, List[int]]` = `None`\n",
    "    A string / integer or a list of strings / integers that represent the column names / indices to drop. When the dataset is a list or a numpy array this parameter must be represented by integers.\n",
    "* **label_columns**: `Union[str, List[str]]` = `None`\n",
    "    The target label(s) of the column(s) in the dataset. These names will be used as the column names for the predictions. The default name is `\"predicted_label_i\"` for the `i` column.\n",
    "* **log_result_set**: `str` = `True`\n",
    "    Whether to log the result set - a DataFrame of the given inputs concatenated with the predictions. Defaulted to `True`.\n",
    "* **result_set_name**: `str` = `\"prediction\"`\n",
    "    The db key to set name of the prediction result and the filename. Defaulted to `\"prediction\"`.\n",
    "* **perform_drift_analysis**: `bool` = `None`\n",
    "    Whether to perform drift analysis between the sample set of the model object to the dataset given. By default, `None`, which means it will perform drift analysis if the model has a sample set statistics.\n",
    "* **sample_set**: `Union[mlrun.DataItem, list, dict, pd.DataFrame, pd.Series, np.ndarray]`\n",
    "    A sample dataset to give to compare the inputs in the drift analysis. The default chosen sample set will always be the one who is set in the model artifact itself.\n",
    "    * Can be passed in `inputs` as either a Dataset artifact / Feature vector URI.\n",
    "    * Or, in `parameters` as a list, dictionary or numpy array.\n",
    "* **drift_threshold**: `float` = `0.7`\n",
    "    The threshold of which to mark drifts. Defaulted to 0.7.\n",
    "* possible_drift_threshold: `float` = `0.5`\n",
    "    The threshold of which to mark possible drifts. Defaulted to 0.5.\n",
    "* **inf_capping**: `float` = `10.0`\n",
    "    The value to set for when it reached infinity. Defaulted to 10.0.\n",
    "* **artifacts_tag**: `str` = `\"\"`\n",
    "    Tag to use for all the artifacts resulted from the function. Defaul,ted to no tag."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. Outputs\n",
    "\n",
    "The outputs are split to two actions the functions can perform:\n",
    "* **Results Prediction** - Will log a dataset artifact named by the `result_set_name` parameter.\n",
    "* **Data Drift Analysis** - Will log a:\n",
    "    * `plotly` artifact named `\"data_drift_table\"` with a visualization of the drifts results and histograms.\n",
    "    * Json file with a drift status and metric per feature.\n",
    "    * Register the overall drift status and metric as results.\n",
    "\n",
    "For more details, see the next chapters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"chapter2\"></a>\n",
    "## 2. Results Prediction\n",
    "\n",
    "The result set is a concatenated dataset of the inputs ($X$) provided and the predictions ($Y$) yielded by the model, so it will be $X | Y$.\n",
    "\n",
    "For example, if the `dataset` given as inputs was:\n",
    "\n",
    "| x1  | x2  | x3  | x4  | x5  |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| ... | ... | ... | ... | ... |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| ... | ... | ... | ... | ... |\n",
    "\n",
    "And the outputs yielded by the model's prediction was:\n",
    "\n",
    "| y1  | y2  |\n",
    "|-----|-----|\n",
    "| ... | ... |\n",
    "| ... | ... |\n",
    "| ... | ... |\n",
    "\n",
    "Then the result set will be:\n",
    "\n",
    "| x1  | x2  | x3  | x4  | x5  | y1  | y2  |\n",
    "|-----|-----|-----|-----|-----|-----|-----|\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| ... | ... | ... | ... | ... | ... | ... |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"chapter3\"></a>\n",
    "## 3. Data Drift Analysis\n",
    "\n",
    "The data drift analysis is done per feature using two distance measure metrics for probability distributions.\n",
    "\n",
    "Let us mark our sample set as $S$ and our inputs as $I$. We will look at one feature out of $n$ features. Assuming the histograms of feature $x$ is split into 20 bins: $b_1,b_2,...,b_{20}$, we will match the feature $x$ histogram of the inputs $I$ ($x_I$) into the same bins (meaning to $x_S$) and compare their distributions using:\n",
    "\n",
    "* Total Variance Distance: $TVD(x_S,x_I) = \\frac{1}{2}\\sum_{b_1}^{b_{20}} {|x_S - x_I|}$\n",
    "* Hellinger Distance: $H(x_S,x_I)=\\sqrt{1-\\sum_{b_1}^{b_{20}}\\sqrt{{x_S\\cdot x_I}}$\n",
    "\n",
    "Our **rule** then is calculating for each $x\\in S: \\frac{H(x_S,x_I)+TVD(x_S,x_I)}{2}$ is smaller then some given thresholds.\n",
    "\n",
    "The outputs of the analysis are:\n",
    "* **Drift table plot** - The results are presented in a `plotly` table artifact named `\"drift_table_plot\"` that shows each feature's statistics and its TVD, Hellinger and KLD (Kullbackâ€“Leibler divergence) results as follows:\n",
    "\n",
    "|        | Count      |            | Mean       |            | Std        |            | Min        |            | Max        |            | Tvd | Hellinger | Kld | Histograms |\n",
    "| ------ | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | --- | --------- | --- |------------|\n",
    "|        | **Sample** | **Input**  | **Sample** | **Input**  | **Sample** | **Input**  | **Sample** | **Input**  | **Sample** | **Input**  |     |           |     |            |\n",
    "| **x1** | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ... | ...       | ... | ...        |\n",
    "| **x2** | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ... | ...       | ... | ...        |\n",
    "| **x3** | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ... | ...       | ... | ...        |\n",
    "\n",
    "* **Features drift results** - A rule metric per feature dictionary is saved in a json file named `\"features_drift_results\"` where each key is a feature and its value is the feature's metric value: `Dict[str, float]`\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "        \"x1\": 0.12,\n",
    "        \"x2\": 0.345,\n",
    "        \"x3\": 0.00678,\n",
    "        ...\n",
    "    }\n",
    "    ```\n",
    "* In addition, two results are being added to summarize the drift analysis:\n",
    "\n",
    "    * `drift_status`: `bool` - A boolean value indicating whether a drift was found.\n",
    "    * `drift_metric`: `float` - The mean of all the features drift metric value (the rule above):\n",
    "        for $n$ features and metric rule $M(x_S,x_I)=\\frac{H(x_S,x_I)+TVD(x_S,x_I)}{2}$, `drift_metric` $=\\frac{\\sum_{x\\in S}M(x_S,x_I)}{n}$\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "        \"drift_status\": True,\n",
    "        \"drift_metric\": 0.81234\n",
    "    }\n",
    "    ```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"chapter4\"></a>\n",
    "## 4. End-to-end Demo\n",
    "\n",
    "We will see an end-to-end example that follows the steps below:\n",
    "1. Generate data.\n",
    "2. Train a model.\n",
    "3. Infer data through the model using `batch_predict` and review the outputs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1. Code review\n",
    "\n",
    "We are using a very simple example of training a decision tree on a binary classification problem. For that we wrote two functions:\n",
    "* `generate_data` - Generate a binary classification data. The data will be split into a *training set* and *data for prediction*. The data for prediction will be drifted in half of its features to showcase the plot later on.\n",
    "* `train` - Train a decision tree classifier on a given data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mlrun: start-code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import mlrun\n",
    "from mlrun.frameworks.sklearn import apply_mlrun\n",
    "\n",
    "\n",
    "@mlrun.handler(outputs=[\"training_set\", \"prediction_set\"])\n",
    "def generate_data(n_samples: int = 5000, n_features: int = 20):\n",
    "    # Generate a classification data:\n",
    "    x, y = make_classification(\n",
    "        n_samples=n_samples, n_features=n_features, n_classes=2\n",
    "    )\n",
    "\n",
    "    # Split the data into a training set and a prediction set:\n",
    "    x_train, x_prediction = x[: n_samples // 2], x[n_samples // 2 :]\n",
    "    y_train = y[: n_samples // 2]\n",
    "\n",
    "    # Initialize dataframes:\n",
    "    features = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    training_set = pd.DataFrame(data=x_train, columns=features)\n",
    "    training_set.insert(\n",
    "        loc=n_features, column=\"label\", value=y_train, allow_duplicates=True\n",
    "    )\n",
    "    prediction_set = pd.DataFrame(data=x_prediction, columns=features)\n",
    "\n",
    "    # Randomly drift half of the features:\n",
    "    drifted_features = prediction_set.sample(n=n_features // 2, axis=\"columns\")\n",
    "    drifted_features += np.random.uniform(low=0, high=10, size=(n_samples // 2, n_features // 2))\n",
    "    prediction_set.update(drifted_features)\n",
    "\n",
    "    return training_set, prediction_set\n",
    "\n",
    "\n",
    "@mlrun.handler()\n",
    "def train(training_set: pd.DataFrame):\n",
    "    # Get the data into x, y:\n",
    "    labels = pd.DataFrame(training_set[\"label\"])\n",
    "    training_set.drop(columns=[\"label\"], inplace=True)\n",
    "\n",
    "    # Initialize a model:\n",
    "    model = DecisionTreeClassifier()\n",
    "\n",
    "    # Apply MLRun:\n",
    "    apply_mlrun(model=model, model_name=\"model\")\n",
    "\n",
    "    # Train:\n",
    "    model.fit(training_set, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mlrun: end-code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2. Run the Example with MLRun\n",
    "\n",
    "First, we will prepare our MLRun functions:\n",
    "1. We will use `mlrun.code_to_function` to turn this demo notebook into an MLRun function we can run.\n",
    "2. We will use `mlrun.import_function` to import the `batch_predict` function ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an MLRun function to run the notebook:\n",
    "demo_function = mlrun.code_to_function(name=\"batch_predict_demo\", kind=\"job\")\n",
    "\n",
    "# Import the `batch_predict` function from the marketplace:\n",
    "batch_predict_function = mlrun.import_function(\"hub://batch_predict\")\n",
    "\n",
    "# Set the desired artifact path:\n",
    "artifact_path = \"./\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we will follow the demo steps as discussed above:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Generate data:\n",
    "generate_data_run = demo_function.run(\n",
    "    handler=\"generate_data\",\n",
    "    artifact_path=artifact_path,\n",
    "    local=True,\n",
    ")\n",
    "\n",
    "# 2. Train a model:\n",
    "train_run = demo_function.run(\n",
    "    handler=\"train\",\n",
    "    artifact_path=artifact_path,\n",
    "    inputs={\"training_set\": generate_data_run.outputs[\"training_set\"]},\n",
    "    local=True,\n",
    ")\n",
    "\n",
    "# 3. Perform batch prediction:\n",
    "batch_predict_run = batch_predict_function.run(\n",
    "    handler=\"predict\",\n",
    "    artifact_path=artifact_path,\n",
    "    inputs={\"dataset\": generate_data_run.outputs[\"prediction_set\"]},\n",
    "    params={\n",
    "        \"model\": train_run.outputs[\"model\"],\n",
    "        \"label_columns\": \"label\",\n",
    "    },\n",
    "    local=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3. Review Outputs\n",
    "\n",
    "We will review the outputs as explained in the notebook above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3.1. Results Prediction\n",
    "\n",
    "First we will showcase the **Result Set**. As we didn't send any name, it's default name will be `\"prediction\"`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_predict_run.outputs(\"prediction\").as_df()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3.2. Data Drift Analysis\n",
    "\n",
    "Second we will review the data drift table plot and the drift results:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_predict_run.outputs(\"drift_table_plot\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_predict_run.status.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}